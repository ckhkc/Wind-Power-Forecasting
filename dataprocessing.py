# -*- coding: utf-8 -*-
"""DataProcessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wbvcaXAcJlEVRqPFmhuBqdvTHC9AAe72
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np

data_turbine = pd.read_csv('/content/drive/MyDrive/data/AIMS 5790/Turbine_Data_Kelmarsh_1_2016-01-03_-_2017-01-01_228_copy.csv')

data_turbine.info()

data_turbine.head(150)

data_turbine.columns

data_turbine.dropna(how = 'all', axis=1, inplace=True)

data_turbine.info()

# 计算保留的阈值：总行数的一半
limit = len(data_turbine) * 0.5

# thresh=limit 表示：这一列至少要有 limit 个非空值，否则就扔掉
# axis=1 表示操作列
data_turbine = data_turbine.dropna(thresh=limit, axis=1)

data_turbine.fillna(value = 0, inplace=True)

data_turbine.info()

df = pd.read_csv('/content/drive/MyDrive/data/Turbine_Data_Kelmarsh_1_2016-01-03_-_2017-01-01_228_copy.csv')  # 假设有时间列

# 2. 确认目标列名（根据你的数据调整）
target = 'Power (kW)'  # 精确匹配列名

# 3. 只保留数值列（排除时间、字符串等）
numeric_df = df.select_dtypes(include=[np.number])

# 确保目标列存在
if target not in numeric_df.columns:
    raise ValueError(f"目标列 '{target}' 不存在，请检查列名")

# 4. 计算与Power的相关性（绝对值），并排序
correlations = numeric_df.corr()[target].abs().sort_values(ascending=False)

# 显示所有相关性（前20个最相关的）
print("与 Power 的相关性（绝对值）Top 50：")
print(correlations.head(50))

data_turbine['Power (kW)'].max()

data_turbine['Power (kW)'].min()

data_turbine[data_turbine['Power (kW)']>0]

import pandas as pd
import numpy as np
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_regression

# 1. 准备数据
# 假设你的数据在 df 变量中 (如果你是读取新文件，请取消下一行的注释)
# data = pd.read_csv('your_file.csv')
data = data_turbine.copy() # 为了不影响原数据，做一个拷贝

# 【关键修改】：处理空值
# F检验和归一化都不能有空值，所以先删除含有空值的行
data = data.dropna()

# 2. 定义 X 和 Y
target_col = 'Power (kW)'

# Y 就是目标列
y = data[target_col]

# X 是除了目标列以外的所有列
# .drop() 用于删除目标列
# .select_dtypes() 用于确保只保留数值类型的列 (F检验只能跑数值)
x = data.drop(columns=[target_col]).select_dtypes(include=[np.number])

# 3. 归一化 (Min-Max Scaling)
# 沿用你原本的写法。注意：如果有列的最大值等于最小值（常数列），分母为0会产生NaN
x = (x - x.min()) / (x.max() - x.min())

# 如果因为除以0产生了新的空值，填充为0
x = x.fillna(0)

# 4. 执行 F检验
# k=5 代表选最好的5个，也可以设为 'all' 查看所有特征
bestfeatures = SelectKBest(score_func=f_regression, k=5)
fit = bestfeatures.fit(x, y)

# 5. 整理结果
dfscores = pd.DataFrame(fit.scores_)
dfcolumns = pd.DataFrame(x.columns)

# 合并结果以便查看
featureScores = pd.concat([dfcolumns, dfscores], axis=1)
featureScores.columns = ['Specs', 'Score']  # 重命名列

# 6. 打印得分最高的 15 个特征
print(featureScores.nlargest(50, 'Score'))

import pandas as pd
import numpy as np
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_regression

# 1. 准备数据
# data = pd.read_csv('your_file.csv')
data = data_turbine.copy()

# 【关键步骤 1】定义保留特征的白名单 (去除了泄露特征)
selected_features_list = [
    # --- 环境与风况数据 ---
    'Wind speed (m/s)',
    'Long Term Wind (m/s)',
    'Density adjusted wind speed (m/s)',
    'Wind direction (°)',
    'Nacelle ambient temperature (°C)',
    'Ambient temperature (converter) (°C)',

    # --- 机组控制与姿态 ---
    'Nacelle position (°)',
    'Yaw bearing angle (°)',
    'Blade angle (pitch position) A (°)',
    'Blade angle (pitch position) B (°)',
    'Blade angle (pitch position) C (°)',
    'Cable windings from calibration point',

    # --- 机械运转状态 ---
    'Rotor speed (RPM)',
    'Generator RPM (RPM)',
    'Gearbox speed (RPM)',
    'Drive train acceleration (mm/ss)',

    # --- 内部组件温度 ---
    'Front bearing temperature (°C)',
    'Rear bearing temperature (°C)',
    'Stator temperature 1 (°C)',
    'Nacelle temperature (°C)',
    'Transformer temperature (°C)',
    'Gear oil inlet temperature (°C)',
    'Gear oil temperature (°C)',
    'Generator bearing rear temperature (°C)',
    'Generator bearing front temperature (°C)',
    'Temp. top box (°C)',
    'Hub temperature (°C)',
    'Rotor bearing temp (°C)',
    'Transformer cell temperature (°C)'
]

# 2. 数据清洗
# F检验不能有空值，先删除空值行
data = data.dropna()

# 3. 定义 X 和 Y
target_col = 'Power (kW)'
y = data[target_col]

# 【关键步骤 2】只提取白名单中存在的列作为 X
# 使用列表推导式，防止某些列在你的数据中不存在而报错
valid_features = [col for col in selected_features_list if col in data.columns]
x = data[valid_features]

print(f"原定筛选特征 {len(selected_features_list)} 个，实际数据中找到 {len(valid_features)} 个。")

# 4. 归一化 (Min-Max Scaling)
x = (x - x.min()) / (x.max() - x.min())
x = x.fillna(0) # 防止常数列归一化产生NaN

# 5. 执行 F检验
# k='all' 表示计算所有特征的分数，方便我们排序观察
bestfeatures = SelectKBest(score_func=f_regression, k='all')
fit = bestfeatures.fit(x, y)

# 6. 整理结果
dfscores = pd.DataFrame(fit.scores_)
dfpvalues = pd.DataFrame(fit.pvalues_)
dfcolumns = pd.DataFrame(x.columns)

# 合并表格
featureScores = pd.concat([dfcolumns, dfscores, dfpvalues], axis=1)
featureScores.columns = ['Specs', 'Score', 'P_Value']

# 7. 打印结果：按分数降序排列
# 打印所有有效的特征，看看谁最重要
print("\n--- F检验特征重要性排名 (已剔除泄露特征) ---")
print(featureScores.nlargest(len(valid_features), 'Score'))

"""天气数据"""

import os

# 这是你提供的 URL (新版 CDS)
url = "https://cds.climate.copernicus.eu/api"

# 这是你提供的 Key (请确认这是否是你自己的 Key)
# 如果这只是示例，请去官网 Profile 页面复制你真正的 Key 替换下面引号里的内容
key = "8416251d-d573-4475-9af5-23afc3081ab1"

# 在 Colab 的系统目录下创建配置文件 .cdsapirc
with open('/root/.cdsapirc', 'w') as f:
    f.write(f'url: {url}\n')
    f.write(f'key: {key}\n')

print("配置成功！配置文件已创建在 /root/.cdsapirc")

!pip install cdsapi

import cdsapi

dataset = "reanalysis-cerra-height-levels"
request = {
    "variable": [
        "temperature",
        "wind_direction",
        "wind_speed"
    ],
    "height_level": ["75_m"],
    "data_type": ["reanalysis"],
    "product_type": ["analysis"],
    "year": ["2016"],
    "month": ["01"],
    "day": [
        "01", "02", "03",
        "04", "05", "06",
        "07", "08", "09",
        "10", "11", "12",
        "13", "14", "15",
        "16", "17", "18",
        "19", "20", "21",
        "22", "23", "24",
        "25", "26", "27",
        "28", "29", "30",
        "31"
    ],
    "time": [
        "00:00", "03:00", "06:00",
        "09:00", "12:00", "15:00",
        "18:00", "21:00"
    ],
    "data_format": "grib"
}

client = cdsapi.Client()
client.retrieve(dataset, request).download()

path = '/content/drive/MyDrive/data/AIMS 5790/2016_1_75m_allday.grib'

!pip install cfgrib

import cfgrib



# 使用 open_datasets 会返回一个列表
datasets = cfgrib.open_datasets(path)

# 查看列表里有几个 dataset
print(f"一共拆分成了 {len(datasets)} 部分")

# 通常 datasets[0] 是其中一个（比如风），datasets[1] 是另一个（比如气温）
# 你可以通过打印来看看哪个是哪个
for i, ds in enumerate(datasets):
    print(f"\n--- 第 {i+1} 部分数据 ---")
    print(ds)

import xarray as xr



# 1. 只读取高度为75的数据 (风速风向)
ds_all = xr.open_dataset(
    path,
    engine='cfgrib',
    backend_kwargs={'filter_by_keys': {'level': 75}}
)
print("数据读取成功：", list(ds_wind.data_vars))

# 2. 只读取高度为75的数据 (气温)
ds_temp = xr.open_dataset(
    path,
    engine='cfgrib',
    backend_kwargs={'filter_by_keys': {'level': 75}}
)
#print("数据读取成功：", list(ds_temp.data_vars))

import shutil

# 源文件路径 (你的网盘路径)
src_path = path

# 目标路径 (Colab 虚拟机的本地临时目录)
dst_path = '/content/temp_data.grib'

print("正在复制文件到本地，请稍候...")
shutil.copy(src_path, dst_path)
print("复制完成！现在读取速度会飞快。")

df.info()

import numpy as np
import xarray as xr

def get_weather_at_point(ds, lat, lon, var_names=['ws', 'wdir', 't']):
    """
    输入：
    ds: xarray 数据集
    lat: 目标纬度
    lon: 目标经度
    var_names: 字符串列表，例如 ['wdir10', 'si10', 't2m']

    输出：
    Pandas DataFrame (包含该点的时间序列，所有变量都在列中)
    """

    # 1. 容错处理：如果用户只传了一个字符串（例如 't2m'），把它变成列表
    if isinstance(var_names, str):
        var_names = [var_names]

    # 检查请求的变量是否在数据集里，避免报错
    # (比如你现在的 dataset 可能只有风的数据，没有温度)
    available_vars = [v for v in var_names if v in ds]
    if not available_vars:
        print(f"警告: 数据集中找不到这些变量 {var_names}。数据集包含: {list(ds.data_vars)}")
        return None

    # 2. 计算距离矩阵 (寻找最近的网格点)
    # 计算量最大的一步，只做一次
    dist = (ds['latitude'] - lat)**2 + (ds['longitude'] - lon)**2

    # 3. 获取最小距离的索引
    min_index = dist.argmin().item()
    y_idx, x_idx = np.unravel_index(min_index, dist.shape)

    print(f"坐标 ({lat}, {lon}) -> 映射到网格点: y={y_idx}, x={x_idx}")

    # 4. 提取数据 (关键改进)
    # 我们先根据坐标切片，得到该点的所有数据
    point_ds = ds.isel(y=y_idx, x=x_idx)

    # 然后只筛选出我们想要的那些变量
    subset = point_ds[available_vars].copy()

    # 5. 单位转换逻辑
    # 如果包含气温 (t2m)，从 开尔文 转 摄氏度
    if 't2m' in subset:
        subset['t2m'] = subset['t2m'] - 273.15

    # 6. 转成 DataFrame
    df = subset.to_dataframe()

    # 7. 清理索引和重命名
    df = df.reset_index()

    # 定义重命名映射表 (让列名更好读)
    rename_map = {
        't2m': 'Temperature_C',      # 2米气温
        'si10': 'Wind_Speed_10m',    # 10米风速
        'wdir10': 'Wind_Direction',  # 10米风向
        'u10': 'Wind_U_Component',
        'v10': 'Wind_V_Component'
    }

    # 只重命名存在的列
    df = df.rename(columns={k: v for k, v in rename_map.items() if k in df.columns})

    return df

df_all = get_weather_at_point(ds_all, 52.24, -0.56)

df_all

"""# .npy"""

import numpy as np

# 1. 加载 .npy 文件
# 请将 'your_file.npy' 替换为你实际的文件路径
file_path = '/content/drive/MyDrive/data/AIMS 5790/wind/X.npy'
data = np.load(file_path)

# 2. 查看数据内容
print("X数据内容：")
print(data)

# 3. 查看数据的基本信息（非常重要，建议先看这个）
print(f"\nX数据形状 (Shape): {data.shape}")  # 比如 (100, 5) 代表100行5列
print(f"X数据类型 (Dtype): {data.dtype}")  # 比如 float64, int32

import numpy as np

# 1. 加载 .npy 文件
# 请将 'your_file.npy' 替换为你实际的文件路径
file_path = '/content/drive/MyDrive/data/AIMS 5790/wind/Y.npy'
data = np.load(file_path)

# 2. 查看数据内容
print("y数据内容：")
print(data)

# 3. 查看数据的基本信息（非常重要，建议先看这个）
print(f"\ny数据形状 (Shape): {data.shape}")  # 比如 (100, 5) 代表100行5列
print(f"y数据类型 (Dtype): {data.dtype}")  # 比如 float64, int32